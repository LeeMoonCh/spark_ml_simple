package com.lc.study.no4features.n2transformers

import org.apache.spark.ml.feature.VectorIndexer


/*
向量下标转换。
 VectorIndexer解决向量数据集中的类别特征索引。它可以自动识别哪些特征是类别型的，并且将原始值转换为类别索引。它的处理流程如下：

​ 1.获得一个向量类型的输入以及maxCategories参数。

​ 2.基于不同特征值的数量来识别哪些特征需要被类别化，其中最多maxCategories个特征需要被类别化。

​ 3.对于每一个类别特征计算0-based（从0开始）类别索引。

​ 4.对类别特征进行索引然后将原始特征值转换为索引。

  索引后的类别特征可以帮助决策树等算法恰当的处理类别型特征，并得到较好结果。

  在下面的例子中，我们读入一个数据集，然后使用VectorIndexer来决定哪些特征需要被作为类别特征，将类别特征转换为他们的索引。

具体例子可以参考以下内容。
其实它的主要作用是，将类别特征，进行转换，大于一定类别数的将会被抛弃，只留下在类别数以下的特征，进行转换。

可以去查看@https://www.e-learn.cn/en/share/3410777 进行观摩学习。

 */


object VectorIndxerDemo extends App{

  import com.lc.study.spark


  //读取格式，将文本数据读成libsvm格式的数据。
  //关于libsvm的数据格式如下：
  /*
  该软件使用的训练数据和检验数据文件格式如下：
    <label> <index1>:<value1> <index2>:<value2> ...
    其中<label> 是训练数据集的目标值，对于分类，它是标识某类的整数（支持多个类）；对于回归，是任意实数。
    <index> 是以1开始的整数，可以是不连续的；<value>；为实数，也就是我们常说的自变量。检验数据文件中
    的label只用于计算准确度或误差，如果它是未知的，只需用一个数填写这一栏，
    也可以空着不填。
   */
  val data = spark.read.format("libsvm").load("data/simple_libsvm_data")

  data.show(false)
  data.printSchema()
  //我取其中一条数据进行观看。
  /*
  692,
  [127,128,129,130,131,154,155,156,157,158,159,181,182,183,184,185,186,187,188,189,207,208,209,210,211,212,213,214,215,216,217,235,236,237,238,239,240,241,242,243,244,245,262,263,264,265,266,267,268,269,270,271,272,273,289,290,291,292,293,294,295,296,297,300,301,302,316,317,318,319,320,321,328,329,330,343,344,345,346,347,348,349,356,357,358,371,372,373,374,384,385,386,399,400,401,412,413,414,426,427,428,429,440,441,442,454,455,456,457,466,467,468,469,470,482,483,484,493,494,495,496,497,510,511,512,520,521,522,523,538,539,540,547,548,549,550,566,567,568,569,570,571,572,573,574,575,576,577,578,594,595,596,597,598,599,600,601,602,603,604,622,623,624,625,626,627,628,629,630,651,652,653,654,655,656,657],
  [51.0,159.0,253.0,159.0,50.0,48.0,238.0,252.0,252.0,252.0,237.0,54.0,227.0,253.0,252.0,239.0,233.0,252.0,57.0,6.0,10.0,60.0,224.0,252.0,253.0,252.0,202.0,84.0,252.0,253.0,122.0,163.0,252.0,252.0,252.0,253.0,252.0,252.0,96.0,189.0,253.0,167.0,51.0,238.0,253.0,253.0,190.0,114.0,253.0,228.0,47.0,79.0,255.0,168.0,48.0,238.0,252.0,252.0,179.0,12.0,75.0,121.0,21.0,253.0,243.0,50.0,38.0,165.0,253.0,233.0,208.0,84.0,253.0,252.0,165.0,7.0,178.0,252.0,240.0,71.0,19.0,28.0,253.0,252.0,195.0,57.0,252.0,252.0,63.0,253.0,252.0,195.0,198.0,253.0,190.0,255.0,253.0,196.0,76.0,246.0,252.0,112.0,253.0,252.0,148.0,85.0,252.0,230.0,25.0,7.0,135.0,253.0,186.0,12.0,85.0,252.0,223.0,7.0,131.0,252.0,225.0,71.0,85.0,252.0,145.0,48.0,165.0,252.0,173.0,86.0,253.0,225.0,114.0,238.0,253.0,162.0,85.0,252.0,249.0,146.0,48.0,29.0,85.0,178.0,225.0,253.0,223.0,167.0,56.0,85.0,252.0,252.0,252.0,229.0,215.0,252.0,252.0,252.0,196.0,130.0,28.0,199.0,252.0,252.0,253.0,252.0,252.0,233.0,145.0,25.0,128.0,252.0,253.0,252.0,141.0,37.0]

   features列的数据，被分成了三列，第1列 为特征总个数  692个。
   第二列 为特征索引。
   第三列 为特征值。
   而，我们的VectorIndexer的作用就是对features列进行计算。
   设置一个最大特征类别个数。比如 127这个特征索引，如果它对于的值小于等于这个值，那么它对于的值就会被转换成索引，而小于这个值得索引对于的值
   不会被进行转换。
   */

//  val indexer = new VectorIndexer()
//      .setInputCol("features")
//      .setOutputCol("indexed")
//      .setMaxCategories(10) //索引类别值个数不能大于10.
  ////
  ////  val indexerModel = indexer.fit(data) //建立模型，需要注意该算法是一个估计器。
  ////
  ////  val categoricalFeatures: Set[Int] = indexerModel.categoryMaps.keys.toSet
  ////  println(s"Chose ${categoricalFeatures.size} " +
  ////    s"categorical features: ${categoricalFeatures.mkString(", ")}")
  ////
  ////  // 使用模型，对数据集进行转换。
  ////  val indexedData = indexerModel.transform(data)
  ////  indexedData.show() //看一下结果。

  //因为官方数据过大，所以我们使用一个较小数据集。
  import spark.implicits._

  import org.apache.spark.ml.linalg.Vectors
  val data1 = spark.createDataFrame(Seq(
    (1,Vectors.sparse(3,Array(0,1,2),Array(2.0,5.0,7.0))),
    (0,Vectors.sparse(3,Array(0,1,2),Array(3.0,5.0,9.0))),
    (0,Vectors.sparse(3,Array(0,1,2),Array(4.0,7.0,9.0))),
    (1,Vectors.sparse(3,Array(0,1,2),Array(2.0,4.0,9.0))),
    (1,Vectors.sparse(3,Array(0,1,2),Array(9.0,5.0,7.0))),
    (0,Vectors.sparse(3,Array(0,1,2),Array(2.0,5.0,9.0))),
    (2,Vectors.sparse(3,Array(0,1,2),Array(3.0,4.0,9.0))),
    (1,Vectors.sparse(3,Array(0,1,2),Array(8.0,4.0,9.0))),
    (0,Vectors.sparse(3,Array(0,1,2),Array(3.0,6.0,2.0))),
    (1,Vectors.sparse(3,Array(0,1,2),Array(5.0,9.0,2.0)))
  )).toDF("label","f")

  data1.show()


  val vectorIndexerModel =  new VectorIndexer()
    .setInputCol("f")
    .setOutputCol("findex")
    .setMaxCategories(5).fit(data1)  //也就是对于特征列表而言，如果某一个索引对于的值得个数大于5个，那么就不会进行值得索引转换


  vectorIndexerModel.transform(data1).show(false)
  /*
+-----+-------------------------+-------------------------+
|label|f                        |findex                   |
+-----+-------------------------+-------------------------+
|1    |(3,[0,1,2],[2.0,5.0,7.0])|(3,[0,1,2],[2.0,1.0,1.0])|
|0    |(3,[0,1,2],[3.0,5.0,9.0])|(3,[0,1,2],[3.0,1.0,2.0])|
|0    |(3,[0,1,2],[4.0,7.0,9.0])|(3,[0,1,2],[4.0,3.0,2.0])|
|1    |(3,[0,1,2],[2.0,4.0,9.0])|(3,[0,1,2],[2.0,0.0,2.0])|
|1    |(3,[0,1,2],[9.0,5.0,7.0])|(3,[0,1,2],[9.0,1.0,1.0])|
|0    |(3,[0,1,2],[2.0,5.0,9.0])|(3,[0,1,2],[2.0,1.0,2.0])|
|2    |(3,[0,1,2],[3.0,4.0,9.0])|(3,[0,1,2],[3.0,0.0,2.0])|
|1    |(3,[0,1,2],[8.0,4.0,9.0])|(3,[0,1,2],[8.0,0.0,2.0])|
|0    |(3,[0,1,2],[3.0,6.0,2.0])|(3,[0,1,2],[3.0,2.0,0.0])|
|1    |(3,[0,1,2],[5.0,9.0,2.0])|(3,[0,1,2],[5.0,4.0,0.0])|
+-----+-------------------------+-------------------------+


我们来分析一下结果，将label抛弃不看，只看f列。
对于 标签0 在全部数据集中它有如下这些值：[2,3,4,9,8,5]
对于 标签1 在全部数据集中它有如下值：[5,7,4,,6,9]
对于 标签2 在全部数据集中有：[7,9,2]
对于我们的设置项5而言，明显，标签0 将不会被转换。 1 和 2的值将会被转换成标签。


   */














}
